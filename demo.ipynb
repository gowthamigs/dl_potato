{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (BATCH_SIZE,IMAGE_SIZE,IMAGE_SIZE, CHANNELS)\n",
    "model = models.Sequential([\n",
    "    rescale_n_resize,\n",
    "    data_augmentation,\n",
    "    layers.Conv2D(filters = 16, kernel_size = (3,3), activation = 'relu', input_shape = input_shape),\n",
    "    layers.MaxPool2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation = 'relu'),\n",
    "    layers.MaxPool2D((2,2)),\n",
    "    layers.Conv2D(128, (3,3), activation = 'relu'),\n",
    "    layers.MaxPool2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation = 'relu'),\n",
    "    layers.MaxPool2D((2,2)),\n",
    "    layers.Conv2D(128, (3,3), activation = 'relu'),\n",
    "    layers.MaxPool2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation = 'relu'),\n",
    "    layers.MaxPool2D((2,2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation = 'relu'),\n",
    "    layers.Dense(64, activation = 'softmax')\n",
    "    \n",
    "])\n",
    "model.build(input_shape=input_shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "             metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data, epochs = EPOCHS, batch_size=BATCH_SIZE, validation_data=val_data, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(train_data)\n",
    "scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for linear algebra\n",
    "import pandas as pd # data preprocessing\n",
    "import tensorflow as tf # deep learning\n",
    "import matplotlib.pyplot as plt # \n",
    "from tensorflow.keras import layers, models # working on layers\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tf.keras.preprocessing.image_dataset_from_directory(\"D:\\\\projects_plc\\\\mini\\\\dataset\",shuffle=True,image_size= (IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                                        batch_size= BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = df.class_names\n",
    "class_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "102*32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, label_batch in df.take(1):\n",
    "    plt.imshow(image_batch[0].numpy().astype(\"uint8\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for image_batch, label_batch in df.take(1):\n",
    "    for i in range(12):\n",
    "        plt.subplot(3,4,i+1)\n",
    "        plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[label_batch[i]])\n",
    "        plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNELS = 3\n",
    "EPOCHS = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "len(df) * train_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_n_test = len(df) - (len(df) * train_size)\n",
    "val_n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_datasets(pc,train_split = 0.8,val_split = 0.1, test_split = 0.1, shuffle = True, shuffle_size = 10000):\n",
    "    if shuffle:\n",
    "        pc = pc.shuffle(shuffle_size,seed = 10)\n",
    "        \n",
    "    pc_size = len(pc) # size of potato_data(68)\n",
    "    train_size = int(train_split*pc_size)\n",
    "    val_size = int(val_split*pc_size)\n",
    "    \n",
    "    train_pc = pc.take(train_size) # taking first 54 batches(out of 68)\n",
    "    val_pc = pc.skip(train_size).take(val_size) # leaving first 54 and taking next 6 batches\n",
    "    test_pc = pc.skip(train_size).skip(val_size) # skipping first 54(train) batch and 6(validation) batch and \n",
    "                                                 #taking left 8 batches for test\n",
    "    \n",
    "    return train_pc, val_pc, test_pc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = split_datasets(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of Data is :{0} \\nBatch Size of Training Data is :{1} \\nBatch Size of Validation Data :{2} \\nBatch Size of Test Data :{3}\"\n",
    "      .format(len(df), len(train_data), len(val_data), len(test_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pc = train_data.cache().shuffle(100).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_pc = val_data.cache().shuffle(100).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_pc = test_data.cache().shuffle(100).prefetch(buffer_size=tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale_n_resize = tf.keras.Sequential([\n",
    "    tf.keras.layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),  # Use the new import path\n",
    "    tf.keras.layers.Rescaling(1./255)  # Use the new import path\n",
    "])\n",
    "\n",
    "# Data Augmentation by flipping and rotating existing images\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(mode='horizontal_and_vertical'),  # Use the new import path\n",
    "    tf.keras.layers.RandomRotation(factor=0.5)  # Use the new import path\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, label_batch in test_data:\n",
    "    predictions = model.predict(image_batch)\n",
    "    y_true.extend(label_batch.numpy())\n",
    "    y_pred.extend(np.argmax(predictions, axis=1))\n",
    "\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = history.history['loss']\n",
    "train_acc = history.history['accuracy']\n",
    "val_loss = history.history['val_loss']\n",
    "val_acc = history.history['val_accuracy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(range(EPOCHS), train_acc, label='Training Accuracy')\n",
    "plt.plot(range(EPOCHS), val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(range(EPOCHS), train_loss, label='Training Loss')\n",
    "plt.plot(range(EPOCHS), val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "# Precision, Recall, and F1 Score plot\n",
    "metrics = [precision, recall, f1]\n",
    "metric_names = ['Precision', 'Recall', 'F1 Score']\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.bar(metric_names, metrics)\n",
    "plt.title('Precision, Recall and F1 Score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,16))\n",
    "for batch_image, batch_label in test_pc.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3,3,i+1)\n",
    "        image = batch_image[i].numpy().astype('uint8')\n",
    "        label = class_names[batch_label[i]]\n",
    "    \n",
    "        plt.imshow(image)\n",
    "    \n",
    "        batch_prediction = model.predict(batch_image)\n",
    "        predicted_class = class_names[np.argmax(batch_prediction[i])]\n",
    "        confidence = round(np.max(batch_prediction[i]) * 100, 2)\n",
    "        \n",
    "        plt.title(f'Actual : {label},\\n Prediction : {predicted_class},\\n Confidence : {confidence}%')\n",
    "    \n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_lf = tf.keras.preprocessing.image_dataset_from_directory(\"D:\\\\projects_plc\\\\mini\\\\dataset\",\n",
    "                                                             image_size=(IMAGE_SIZE,IMAGE_SIZE))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, label_batch in unk_lf.take(1):\n",
    "    for i in range(6):\n",
    "        plt.subplot(2,3,i+1)\n",
    "        plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "for image_batch, label_batch in unk_lf.take(1):\n",
    "    for i in range(6):\n",
    "        ax = plt.subplot(2,3,i+1)\n",
    "        image = image_batch[i].numpy().astype(\"uint8\")\n",
    "        label = class_names[label_batch[i]]\n",
    "        \n",
    "        plt.imshow(image)\n",
    "\n",
    "                \n",
    "        unk_lf_pred = model.predict(image_batch)\n",
    "        pred_class = class_names[np.argmax(unk_lf_pred[i])]\n",
    "        confidence = round(np.max(unk_lf_pred[i])*100, 2)\n",
    "        \n",
    "        plt.title(f'Predicted Class : {pred_class}, \\n Confidence : {confidence}%')\n",
    "        \n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import keras\n",
    "# keras.saving.save_model('potato.keras')\n",
    "model.save('potato.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minip",
   "language": "python",
   "name": "minip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
